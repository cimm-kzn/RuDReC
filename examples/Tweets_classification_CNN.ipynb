{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and validation sets are obtained from the second task of [SMM4H ’20 Shared Task](https://healthlanguageprocessing.org/smm4h-sharedtask-2020/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fasttext model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Russian language, you can use the [Fasttext model](https://drive.google.com/file/d/1su3IYY1avcj95tez69JI8f5qsTng72-I/view?usp=sharing) pretrained on the raw part of the RuDReC corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting train and dev sets into train, dev, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_COLUMNS = [\"class\", \"tweet\"]\n",
    "\n",
    "training_path = r\"task_2_ru_data/task2_ru_training.tsv\"\n",
    "val_path = r\"task_2_ru_data/task2_ru_validation.tsv\"\n",
    "res_dataset_dir = r\"corpus_normalized/\"\n",
    "if not os.path.exists(res_dataset_dir):\n",
    "    os.makedirs(res_dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(training_path, sep=\"\\t\", encoding=\"utf-8\")\n",
    "train_df = train_df[KEEP_COLUMNS]\n",
    "# We use the old validation set as a new test set\n",
    "test_df = pd.read_csv(val_path, sep=\"\\t\", encoding=\"utf-8\")\n",
    "test_df = test_df[KEEP_COLUMNS]\n",
    "train_df, dev_df, _, _ = \\\n",
    "    train_test_split(train_df, train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "train_positive_class_df = train_df[train_df['class'] == 1]\n",
    "train_negative_class_df = train_df[train_df['class'] == 0]\n",
    "num_positive_examples = train_positive_class_df.shape[0]\n",
    "# For training set, we take the same amount of positive and negative examples\n",
    "train_negative_class_df = train_negative_class_df.sample(num_positive_examples, )\n",
    "# Concatenating positive and negative examples and shuffling the training set\n",
    "class_normalized_train_df = pd.concat([train_positive_class_df, train_negative_class_df]).sample(frac=1)\n",
    "\n",
    "\n",
    "out_train_path = os.path.join(res_dataset_dir, \"train.tsv\")\n",
    "out_test_path = os.path.join(res_dataset_dir, \"test.tsv\")\n",
    "out_dev_path = os.path.join(res_dataset_dir, \"dev.tsv\")\n",
    "\n",
    "class_normalized_train_df.to_csv(out_train_path, sep=\"\\t\", encoding=\"utf-8\", index=False, )\n",
    "test_df.to_csv(out_test_path, sep=\"\\t\", encoding=\"utf-8\", index=False)\n",
    "dev_df.to_csv(out_dev_path, sep=\"\\t\", encoding=\"utf-8\", index=False, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CNN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture is adopted from:\n",
    "\n",
    "https://github.com/ShawnyXiao/TextClassification-Keras/tree/master/model/TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Embedding, Dense, Conv1D, GlobalMaxPooling1D, Concatenate\n",
    "\n",
    "class TextCNN(Model):\n",
    "\n",
    "    def __init__(self,\n",
    "                 maxlen,\n",
    "                 max_features,\n",
    "                 embedding_dims,\n",
    "                 kernel_sizes=[3, 4, 5],\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid',\n",
    "                 embedding_weights=None):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "        self.embedding = Embedding(self.max_features, self.embedding_dims,\n",
    "                                   input_length=self.maxlen, weights=[embedding_weights], )\n",
    "        self.convs = []\n",
    "        self.max_poolings = []\n",
    "        for kernel_size in self.kernel_sizes:\n",
    "            self.convs.append(Conv1D(128, kernel_size, activation='relu'))\n",
    "            self.max_poolings.append(GlobalMaxPooling1D())\n",
    "        self.classifier = Dense(self.class_num, activation=self.last_activation)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if len(inputs.get_shape()) != 2:\n",
    "            raise ValueError('The rank of inputs of TextCNN must be 2, but now is %d' % len(inputs.get_shape()))\n",
    "        if inputs.get_shape()[1] != self.maxlen:\n",
    "            raise ValueError(\n",
    "                'The maxlen of inputs of TextCNN must be %d, but now is %d' % (self.maxlen, inputs.get_shape()[1]))\n",
    "        # Embedding part can try multichannel as same as origin paper\n",
    "        embedding = self.embedding(inputs)\n",
    "        convs = []\n",
    "        for i in range(len(self.kernel_sizes)):\n",
    "            c = self.convs[i](embedding)\n",
    "            c = self.max_poolings[i](c)\n",
    "            convs.append(c)\n",
    "        x = Concatenate()(convs)\n",
    "        output = self.classifier(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras_preprocessing import sequence\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from preprocessing import clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EMBEDDINGS_DIM = 200\n",
    "CLASSIFIER_TRAIN_EPOCHS = 10\n",
    "CLASSIFICATION_THRESHOLD = 0.5\n",
    "\n",
    "train_path = r\"corpus_normalized/train.tsv\"\n",
    "dev_path = r\"corpus_normalized/dev.tsv\"\n",
    "test_path = r\"corpus_normalized/test.tsv\"\n",
    "fasttext_model_path = r\"fasttext_training/fasttext_model_2.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "train_df = pd.read_csv(train_path, sep='\\t', encoding=\"utf-8\",)\n",
    "dev_df = pd.read_csv(dev_path, sep='\\t', encoding=\"utf-8\",)\n",
    "test_df = pd.read_csv(test_path, sep='\\t', encoding=\"utf-8\",)\n",
    "# Loading pretrained fastext model\n",
    "fasttext_model = fasttext.load_model(fasttext_model_path)\n",
    "\n",
    "# Extracting tweet texts\n",
    "train_tweet_texts = train_df.tweet.values\n",
    "test_tweet_texts = test_df.tweet.values\n",
    "dev_tweet_texts = dev_df.tweet.values\n",
    "\n",
    "# Extracting tweet labels\n",
    "train_labels = train_df['class'].values\n",
    "test_labels = test_df['class'].values\n",
    "dev_labels = dev_df['class'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Preprocessing is adopted from:\n",
    "\n",
    "https://github.com/akutuzov/webvectors/blob/master/preprocessing/modular_processing/unify.py\n",
    "\n",
    "We unify letters to decrease the size of dictionary. We also unify and remove all punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def list_replace(search, replacement, text):\n",
    "    \"\"\"\n",
    "    Replaces all symbols of text which are present\n",
    "    in the search string with the replacement string.\n",
    "    \"\"\"\n",
    "    search = [el for el in search if el in text]\n",
    "    for c in search:\n",
    "        text = text.replace(c, replacement)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    text = list_replace \\\n",
    "        ('\\u00AB\\u00BB\\u2039\\u203A\\u201E\\u201A\\u201C\\u201F\\u2018\\u201B\\u201D\\u2019', '\\u0022', text)\n",
    "\n",
    "    text = list_replace \\\n",
    "        ('\\u2012\\u2013\\u2014\\u2015\\u203E\\u0305\\u00AF', '\\u2003\\u002D\\u002D\\u2003', text)\n",
    "\n",
    "    text = list_replace('\\u2010\\u2011', '\\u002D', text)\n",
    "\n",
    "    text = list_replace \\\n",
    "            (\n",
    "            '\\u2000\\u2001\\u2002\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200A\\u200B\\u202F\\u205F\\u2060\\u3000',\n",
    "            '\\u2002', text)\n",
    "\n",
    "    text = re.sub('\\u2003\\u2003', '\\u2003', text)\n",
    "    text = re.sub('\\t\\t', '\\t', text)\n",
    "\n",
    "    text = list_replace \\\n",
    "            (\n",
    "            '\\u02CC\\u0307\\u0323\\u2022\\u2023\\u2043\\u204C\\u204D\\u2219\\u25E6\\u00B7\\u00D7\\u22C5\\u2219\\u2062',\n",
    "            '.', text)\n",
    "\n",
    "    text = list_replace('\\u2217', '\\u002A', text)\n",
    "\n",
    "    text = list_replace('…', '...', text)\n",
    "\n",
    "    text = list_replace('\\u00C4', 'A', text)\n",
    "    text = list_replace('\\u00E4', 'a', text)\n",
    "    text = list_replace('\\u00CB', 'E', text)\n",
    "    text = list_replace('\\u00EB', 'e', text)\n",
    "    text = list_replace('\\u1E26', 'H', text)\n",
    "    text = list_replace('\\u1E27', 'h', text)\n",
    "    text = list_replace('\\u00CF', 'I', text)\n",
    "    text = list_replace('\\u00EF', 'i', text)\n",
    "    text = list_replace('\\u00D6', 'O', text)\n",
    "    text = list_replace('\\u00F6', 'o', text)\n",
    "    text = list_replace('\\u00DC', 'U', text)\n",
    "    text = list_replace('\\u00FC', 'u', text)\n",
    "    text = list_replace('\\u0178', 'Y', text)\n",
    "    text = list_replace('\\u00FF', 'y', text)\n",
    "    text = list_replace('\\u00DF', 's', text)\n",
    "    text = list_replace('\\u1E9E', 'S', text)\n",
    "    # Removing punctuation\n",
    "    text = list_replace(',.[]{}()=+-−*&^%$#@!~;:§/\\|\\?\"\\n', ' ', text)\n",
    "    # Replacing all numbers with masks\n",
    "    text = list_replace('0123456789', 'x', text)\n",
    "\n",
    "    currencies = list \\\n",
    "            (\n",
    "            '\\u20BD\\u0024\\u00A3\\u20A4\\u20AC\\u20AA\\u2133\\u20BE\\u00A2\\u058F\\u0BF9\\u20BC\\u20A1\\u20A0\\u20B4\\u20A7\\u20B0\\u20BF\\u20A3\\u060B\\u0E3F\\u20A9\\u20B4\\u20B2\\u0192\\u20AB\\u00A5\\u20AD\\u20A1\\u20BA\\u20A6\\u20B1\\uFDFC\\u17DB\\u20B9\\u20A8\\u20B5\\u09F3\\u20B8\\u20AE\\u0192'\n",
    "        )\n",
    "\n",
    "    alphabet = list \\\n",
    "            (\n",
    "            '\\t\\r абвгдеёзжийклмнопрстуфхцчшщьыъэюяАБВГДЕЁЗЖИЙКЛМНОПРСТУФХЦЧШЩЬЫЪЭЮЯabcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ')\n",
    "\n",
    "    allowed = set(currencies + alphabet)\n",
    "\n",
    "    cleaned_text = [sym for sym in text if sym in allowed]\n",
    "    cleaned_text = ''.join(cleaned_text)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 0\n",
    "# Preprocessing training tweets\n",
    "cleaned_train_texts = []\n",
    "for tweet_text in train_tweet_texts:\n",
    "    cleaned_text = clean_text(tweet_text).lower()\n",
    "    split_cleaned_text = cleaned_text.split()\n",
    "    # Estimating max length of all training tweets in tokens\n",
    "    if len(split_cleaned_text) > maxlen:\n",
    "        maxlen = len(split_cleaned_text)\n",
    "    cleaned_train_texts.append(\" \".join(split_cleaned_text))\n",
    "    \n",
    "# Preprocessing test tweets\n",
    "cleaned_test_texts = []\n",
    "for tweet_text in test_tweet_texts:\n",
    "    cleaned_text = clean_text(tweet_text)\n",
    "    cleaned_test_texts.append(\" \".join(cleaned_text.split()))\n",
    "    \n",
    "# Preprocessing validation tweets\n",
    "cleaned_dev_texts = []\n",
    "for tweet_text in dev_tweet_texts:\n",
    "    cleaned_text = clean_text(tweet_text)\n",
    "    cleaned_dev_texts.append(\" \".join(cleaned_text.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding each tweet as a sequence of token ids. Initializing an embedding matrix using pretrained Fasttext model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(cleaned_train_texts + cleaned_test_texts + cleaned_dev_texts)\n",
    "# Converting texts to lists of ids\n",
    "word_seq_train = tokenizer.texts_to_sequences(cleaned_train_texts)\n",
    "word_seq_test = tokenizer.texts_to_sequences(cleaned_test_texts)\n",
    "word_seq_dev = tokenizer.texts_to_sequences(cleaned_dev_texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Padding too short tweet texts with '0's\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=maxlen)\n",
    "word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=maxlen)\n",
    "word_seq_dev = sequence.pad_sequences(word_seq_dev, maxlen=maxlen)\n",
    "\n",
    "dictionary_size = len(word_index.keys())\n",
    "# 0-th token of embedding matrix is a padding token\n",
    "embedding_matrix = np.zeros((dictionary_size + 1, EMBEDDINGS_DIM))\\\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = fasttext_model.get_word_vector((word))\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling model\n",
    "\n",
    "We add early stopping callback and keep model weights from the epoch with the highest validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextCNN(maxlen, dictionary_size + 1, EMBEDDINGS_DIM, embedding_weights=embedding_matrix)\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'], )\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3 , mode='max', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 950 samples, validate on 609 samples\n",
      "Epoch 1/10\n",
      "950/950 [==============================] - 7s 7ms/sample - loss: 0.6264 - accuracy: 0.6558 - val_loss: 0.5301 - val_accuracy: 0.7718\n",
      "Epoch 2/10\n",
      "950/950 [==============================] - 3s 3ms/sample - loss: 0.3917 - accuracy: 0.8779 - val_loss: 0.5530 - val_accuracy: 0.7258\n",
      "Epoch 3/10\n",
      "950/950 [==============================] - 3s 3ms/sample - loss: 0.2768 - accuracy: 0.9389 - val_loss: 0.6198 - val_accuracy: 0.6864\n",
      "Epoch 4/10\n",
      "950/950 [==============================] - 3s 4ms/sample - loss: 0.2086 - accuracy: 0.9589 - val_loss: 0.4515 - val_accuracy: 0.7865\n",
      "Epoch 5/10\n",
      "950/950 [==============================] - 3s 4ms/sample - loss: 0.1422 - accuracy: 0.9842 - val_loss: 0.3645 - val_accuracy: 0.8243\n",
      "Epoch 6/10\n",
      "950/950 [==============================] - 3s 3ms/sample - loss: 0.1033 - accuracy: 0.9958 - val_loss: 0.5210 - val_accuracy: 0.7553\n",
      "Epoch 7/10\n",
      "950/950 [==============================] - 3s 4ms/sample - loss: 0.0726 - accuracy: 0.9989 - val_loss: 0.4546 - val_accuracy: 0.7833\n",
      "Epoch 8/10\n",
      "950/950 [==============================] - 3s 3ms/sample - loss: 0.0525 - accuracy: 1.0000 - val_loss: 0.4358 - val_accuracy: 0.7997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ca47b38>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(word_seq_train, train_labels,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=CLASSIFIER_TRAIN_EPOCHS,\n",
    "              callbacks=[early_stopping, ],\n",
    "              validation_data=(word_seq_dev, dev_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting labels for dev and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_prob = model.predict(word_seq_test)\n",
    "predicted_test_labels = []\n",
    "predicted_dev_prob = model.predict(word_seq_dev)\n",
    "predicted_dev_labels = []\n",
    "\n",
    "for subarray in predicted_test_prob:\n",
    "    label = 1 if subarray[0] >= CLASSIFICATION_THRESHOLD else 0\n",
    "    predicted_test_labels.append(label)\n",
    "\n",
    "for subarray in predicted_dev_prob:\n",
    "    label = 1 if subarray[0] >= CLASSIFICATION_THRESHOLD else 0\n",
    "    predicted_dev_labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating dev and test precision, recall, f1-score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev:\n",
      "Precision: 0.304\n",
      "Recall: 0.6551724137931034\n",
      "F-measure: 0.4153005464480874\n",
      "Test:\n",
      "Precision: 0.28165374677002586\n",
      "Recall: 0.8195488721804511\n",
      "F-measure: 0.4192307692307693\n",
      "\n",
      "Test classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.80      0.88      1389\n",
      "           1       0.28      0.82      0.42       133\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      1522\n",
      "   macro avg       0.63      0.81      0.65      1522\n",
      "weighted avg       0.92      0.80      0.84      1522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_precision = precision_score(dev_labels, predicted_dev_labels, )\n",
    "dev_recall = recall_score(dev_labels, predicted_dev_labels, )\n",
    "dev_f_measure = f1_score(dev_labels, predicted_dev_labels, )\n",
    "print(f\"Dev:\\nPrecision: {dev_precision}\\n\"\n",
    "        f\"Recall: {dev_recall}\\nF-measure: {dev_f_measure}\")\n",
    "\n",
    "test_precision = precision_score(test_labels, predicted_test_labels, )\n",
    "test_recall = recall_score(test_labels, predicted_test_labels, )\n",
    "test_f_measure = f1_score(test_labels, predicted_test_labels, )\n",
    "print(f\"Test:\\nPrecision: {test_precision}\\n\"\n",
    "        f\"Recall: {test_recall}\\nF-measure: {test_f_measure}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
